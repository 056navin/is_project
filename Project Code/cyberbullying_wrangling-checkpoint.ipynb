{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a6ab36c23ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# dependencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hate Speech Twitter Annotations\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for labeled tweet ids\n",
    "url = 'https://github.com/ZeerakW/hatespeech/archive/master.zip'\n",
    "\n",
    "# use requests to establish connection\n",
    "response = requests.get(url)\n",
    "\n",
    "# create folder 'data'\n",
    "folder_name = 'data'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# download zip file\n",
    "with open(os.path.join(folder_name, 'master'), mode = 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# extract zipfile\n",
    "with zipfile.ZipFile('data/master.zip', 'r') as zipf:\n",
    "    zipf.extractall(os.path.join('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv file of labeled tweet ids\n",
    "labeled_ids = pd.read_csv('data/hatespeech-master/NAACL_SRW_2016.csv', names = ['id', 'label'])\n",
    "\n",
    "labeled_ids.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert secret tokens and keys from Twitter Developer account\n",
    "consumer_key = '###'\n",
    "consumer_secret = '###'\n",
    "access_token = '###'\n",
    "access_secret = '###'\n",
    "\n",
    "# authenticate as per tweepy docs\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "# create api object\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init counter to keep track of tweets collected and of the failed ids\n",
    "i = 0\n",
    "j = 0\n",
    "failed_ids = []\n",
    "\n",
    "# open file to write json objects from api\n",
    "with open('data/tweets.txt', 'w') as outfile:\n",
    "    for _ in labeled_ids.id:\n",
    "        \n",
    "        # try-except block since few tweet IDs in the archive may have been deleted\n",
    "        try: \n",
    "            tweet = api.get_status(_, tweet_mode = 'extended')\n",
    "        except:\n",
    "            failed_ids.append(_)\n",
    "            j = j+1\n",
    "            print(f'Failed: {_} | {j} of {len(labeled_ids.id)}')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # print the number of tweets collected\n",
    "        print(f'Success: {_} | {i} of {len(labeled_ids.id)}')\n",
    "        i = i+1\n",
    "        \n",
    "        # dump the json object corresponding to the tweet collected from the api\n",
    "        json.dump(tweet._json, outfile)\n",
    "        outfile.write('\\n')\n",
    "print(f'Number of Successful Tweets Querried: {i}')\n",
    "print(f'Number of Failed Queries: {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the json data and store it in a list\n",
    "data = []\n",
    "with open('data/tweets.txt') as f:    \n",
    "        for line in f:         \n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df_api = pd.DataFrame(data)\n",
    "\n",
    "#select columns of interest\n",
    "columns_of_interest = ['id', 'full_text']\n",
    "df_api = df_api[columns_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the dataframes with ID's and tweets\n",
    "df = labeled_ids.merge(df_api, left_on = 'id', right_on = 'id', how = 'left');\n",
    "\n",
    "# drop the id's whose tweets could not be retrieved\n",
    "df.dropna(how = 'any', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels to binary classes\n",
    "df['label'] = df.label.map({'none': 'Non-offensive', 'sexism': 'Offensive', 'racism': 'Offensive'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "df.to_csv('labeled_tweets.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hate Speech and Offensive Language Detection\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for GitHub dataset\n",
    "url = 'https://github.com/t-davidson/hate-speech-and-offensive-language/raw/master/data/labeled_data.csv'\n",
    "\n",
    "# use requests to establish connection\n",
    "response = requests.get(url)\n",
    "\n",
    "# create folder 'data'\n",
    "folder_name = 'data'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# download zip file\n",
    "with open(os.path.join(folder_name, 'public_data.csv'), mode = 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df2 = pd.read_csv('data/public_data.csv')\n",
    "\n",
    "# select only the column 'class'\n",
    "df2 = df2.iloc[:, -2:]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map classes to labels\n",
    "df2['class'] = df2['class'].map({0: 'Offensive', 1: 'Offensive', 2: 'Non-offensive'})\n",
    "\n",
    "df2.rename(columns = {'class': 'label', 'tweet': 'full_text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "df2.to_csv('public_data_labeled.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
