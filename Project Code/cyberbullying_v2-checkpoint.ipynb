{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Cyberbullying in Tweets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from time import time\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Analyzing the data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/labeled-tweets/labeled_tweets.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5a9aa06cde45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_scraped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/labeled-tweets/labeled_tweets.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_public\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/public-dataset/public_data_labeled.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../input/labeled-tweets/labeled_tweets.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df_scraped = pd.read_csv('../input/labeled-tweets/labeled_tweets.csv')\n",
    "df_public = pd.read_csv('../input/public-dataset/public_data_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped.drop_duplicates(inplace = True)\n",
    "df_scraped.drop('id', axis = 'columns', inplace = True)\n",
    "\n",
    "df_public.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_scraped, df_public])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "sorted_counts = df['label'].value_counts()\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
    "       autopct='%1.1f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n",
    "        colors = sns.color_palette(\"Paired\")[7:])\n",
    "plt.text(x = -0.35, y = 0, s = 'Total Tweets: {}'.format(df.shape[0]))\n",
    "plt.title('Distribution of Tweets in the Dataset', fontsize = 16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.label.map({'Offensive': 1, 'Non-offensive': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a training and predicting Pipeline\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['full_text'], \n",
    "                                                    df['label'], \n",
    "                                                    random_state=42)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer(stop_words = 'english', lowercase = True)\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(learner_list, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    # Get length of Training Data:\n",
    "    size = len(y_train)\n",
    "    \n",
    "    results = {}\n",
    "    final_results = []\n",
    "    \n",
    "    for learner in learner_list:\n",
    "        \n",
    "        # Store the learner name:\n",
    "        results['Algorithm'] = learner.__class__.__name__\n",
    "\n",
    "        # Fit the learner:\n",
    "        start = time() # Get start time\n",
    "        print(\"Training {}\".format(learner.__class__.__name__))\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        end = time() # Get end time\n",
    "\n",
    "        # Store the training time\n",
    "        results['Training Time'] = end - start\n",
    "\n",
    "        start = time() # Get start time\n",
    "        predictions_test = learner.predict(X_test)\n",
    "        predictions_train = learner.predict(X_train)\n",
    "        end = time() # Get end time\n",
    "\n",
    "        # Store the prediction time\n",
    "        results['Prediction Time'] = end - start\n",
    "\n",
    "        # Compute the Accuracy on Test Set\n",
    "        results['Accuracy: Test'] = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the Accuracy on Training Set\n",
    "        results['Accuracy: Train'] = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "        # Compute the F1 Score on Test Set\n",
    "        results['F1 Score: Test'] = f1_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the F1 Score on Training Set\n",
    "        results['F1 Score: Train'] = f1_score(y_train, predictions_train)\n",
    "\n",
    "        # Compute the Precision on Test Set\n",
    "        results['Precision: Test'] = precision_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the Precision on Training Set\n",
    "        results['Precision: Train'] = precision_score(y_train, predictions_train)\n",
    "\n",
    "        # Compute the Recall on Test Set\n",
    "        results['Recall: Test'] = recall_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the Recall on Training Set\n",
    "        results['Recall: Train'] = recall_score(y_train, predictions_train)\n",
    "\n",
    "        # Success\n",
    "        print(\"Training {} finished in {:.2f} sec\".format(learner.__class__.__name__, results['Training Time']))\n",
    "        print('----------------------------------------------------')\n",
    "        \n",
    "        final_results.append(results.copy())\n",
    "    # Return a dataframe of the results\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put Algorithms in Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of models\n",
    "models = [MultinomialNB(), DecisionTreeClassifier(), LinearSVC(), AdaBoostClassifier(), \n",
    "          RandomForestClassifier(), BaggingClassifier(),\n",
    "         LogisticRegression(), SGDClassifier(), KNeighborsClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = pipeline(models, training_data, y_train, testing_data, y_test)\n",
    "results = pd.DataFrame(re)\n",
    "results = results.reindex(columns = ['Algorithm', 'Accuracy: Test', 'Precision: Test', 'Recall: Test', 'F1 Score: Test', 'Prediction Time',\n",
    "                          'Accuracy: Train', 'Precision: Train', 'Recall: Train', 'F1 Score: Train', 'Training Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reindex(columns = ['Algorithm', 'Accuracy: Test', 'Precision: Test', 'Recall: Test', 'F1 Score: Test', 'Prediction Time',\n",
    "                          'Accuracy: Train', 'Precision: Train', 'Recall: Train', 'F1 Score: Train', 'Training Time'])\n",
    "\n",
    "results.sort_values(by = 'F1 Score: Test', inplace = True, ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the results of the Pipeline\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.describe().loc[['min', 'max'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the best metric scores\n",
    "best_acc = results[results['Accuracy: Test'] == results['Accuracy: Test'].max()]\n",
    "best_f1 = results[results['F1 Score: Test'] == results['F1 Score: Test'].max()]\n",
    "best_precision = results[results['Precision: Test'] == results['Precision: Test'].max()]\n",
    "best_recall = results[results['Recall: Test'] == results['Recall: Test'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize = (17, 7))\n",
    "\n",
    "barWidth = 0.17\n",
    " \n",
    "# set height of bar\n",
    "bars1 = results['Accuracy: Test']\n",
    "bars2 = results['F1 Score: Test']\n",
    "bars3 = results['Precision: Test']\n",
    "bars4 = results['Recall: Test']\n",
    "\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "\n",
    " \n",
    "# Make the plot\n",
    "pal = sns.color_palette()\n",
    "plt.bar(r1, bars1, color= pal[0], width=barWidth, edgecolor='white', label='Test Accuracy')\n",
    "plt.bar(r2, bars2, color= pal[1], width=barWidth, edgecolor='white', label='F1 Score')\n",
    "plt.bar(r3, bars3, color= pal[2], width=barWidth, edgecolor='white', label='Precision')\n",
    "plt.bar(r4, bars4, color= pal[4], width=barWidth, edgecolor='white', label='Recall')\n",
    "\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Algorithm', fontweight='bold', fontsize = 13)\n",
    "plt.ylabel('Score', fontweight = 'bold', fontsize = 13)\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], results['Algorithm'], rotation = 15, fontsize = 11)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend(fontsize = 13)\n",
    "\n",
    "textstr = '\\n'.join(['Best Accuracy: {:.3f} - {}'.format(best_acc['Accuracy: Test'].values[0], best_acc['Algorithm'].values[0]), \n",
    "                     'Best F1 Score: {:.3f} - {}'.format(best_f1['F1 Score: Test'].values[0], best_f1['Algorithm'].values[0]),\n",
    "                   'Best Precision: {:.3f} - {}'.format(best_precision['Precision: Test'].values[0], best_precision['Algorithm'].values[0]), \n",
    "                    'Best Recall: {:.3f} - {}'.format(best_recall['Recall: Test'].values[0], best_recall['Algorithm'].values[0])])\n",
    "props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)\n",
    "\n",
    "#place a text box\n",
    "plt.text(9.2, 1, textstr, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.title('Classification Summary of Algorithms', fontweight = 'bold', fontsize = 17);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the best/worst performance time\n",
    "best_train_time = results[results['Training Time'] == results['Training Time'].min()]\n",
    "worst_train_time = results[results['Training Time'] == results['Training Time'].max()]\n",
    "best_prediction_time = results[results['Prediction Time'] == results['Prediction Time'].min()]\n",
    "worst_prediction_time = results[results['Prediction Time'] == results['Prediction Time'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "barWidth = 0.17\n",
    " \n",
    "# set height of bar\n",
    "bars1 = results['Training Time']\n",
    "bars2 = results['Prediction Time']\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, color= pal[0], width=barWidth, edgecolor='white', label='Training Time')\n",
    "plt.bar(r2, bars2, color= pal[1], width=barWidth, edgecolor='white', label='Prediction Time')\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Algorithm', fontweight='bold', fontsize = 13)\n",
    "plt.ylabel('Time (seconds)', fontweight = 'bold', fontsize = 13)\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], results['Algorithm'], rotation = 15, fontsize = 11)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend(fontsize = 13)\n",
    "\n",
    "textstr = '\\n'.join(('Best Training Time: {:.3f} - {}'.format(best_train_time['Training Time'].values[0], best_train_time['Algorithm'].values[0]), \n",
    "                     'Worst Training Time: {:.3f} - {}'.format(worst_train_time['Training Time'].values[0], worst_train_time['Algorithm'].values[0]),\n",
    "                   'Best Prediction Time: {:.3f} - {}'.format(best_prediction_time['Training Time'].values[0], best_prediction_time['Algorithm'].values[0]), \n",
    "                    'Worst Prediction Time: {:.3f} - {}'.format(worst_prediction_time['Training Time'].values[0], worst_prediction_time['Algorithm'].values[0])))\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)\n",
    "\n",
    "#place a text box\n",
    "plt.text(9, 36, textstr, fontsize=14,  bbox=props)\n",
    "\n",
    "plt.title('Time Complexity of Algorithms', fontweight = 'bold', fontsize = 17);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Algorithms Bagging, SGD, Logistic Regression and Decision Tree and Random Forest and LinearSVC have more or less similar performance. We will tune the hyperparameters of these algorithms. However, the training time for Bagging is very high as compared to the others. Hence we drop it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tuning(clf, param_dict, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # make scorer object\n",
    "    scorer = make_scorer(f1_score)\n",
    "\n",
    "    # perform Grid Search for Parameters\n",
    "    grid_obj = GridSearchCV(estimator = clf,\n",
    "                           param_grid = param_dict,\n",
    "                           scoring = scorer,\n",
    "                           cv = 5)\n",
    "\n",
    "    grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "    # Get the estimator\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "\n",
    "    # Make predictions using the unoptimized and model\n",
    "    predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "    best_predictions = best_clf.predict(X_test)\n",
    "    \n",
    "    # Report the before-and-afterscores\n",
    "    print(clf.__class__.__name__)\n",
    "    print(\"\\nOptimized Model\\n------\")\n",
    "    print(\"Best Parameters: {}\".format(grid_fit.best_params_))\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "    print(\"F1-score: {:.4f}\".format(f1_score(y_test, best_predictions)))\n",
    "    print(\"Precision: {:.4f}\".format(precision_score(y_test, best_predictions)))\n",
    "    print(\"Recall: {:.4f}\".format(recall_score(y_test, best_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for parameters\n",
    "param_grid = {\n",
    "    'alpha' : [0.095, 0.0002, 0.0003],\n",
    "    'max_iter' : [2500, 3000, 4000]\n",
    "}\n",
    "\n",
    "clf_sgd = SGDClassifier()\n",
    "\n",
    "param_tuning(clf_sgd, param_grid, training_data, y_train, testing_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for parameters\n",
    "param_grid = {\n",
    "    'C': [1, 1.2, 1.3, 1.4]\n",
    "}\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "\n",
    "param_tuning(clf_lr, param_grid, training_data, y_train, testing_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'min_samples_split': [2, 5, 8],\n",
    "    'min_samples_leaf': [1, 2, 5, 8]\n",
    "}\n",
    "\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "param_tuning(clf_dt, param_grid, training_data, y_train, testing_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50,150],\n",
    "    'min_samples_leaf': [1, 5],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "param_tuning(clf_rf, param_grid, training_data, y_train, testing_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.25, 0.5, 0.75, 1, 1.2]\n",
    "}\n",
    "\n",
    "clf_linsvc = LinearSVC()\n",
    "\n",
    "param_tuning(clf_linsvc, param_grid, training_data, y_train, testing_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "***\n",
    "We found Stochastic Gradient to be the best suited model for our data. We achieved the following performance parameters:\n",
    "-\tAccuracy: 92.81 %\n",
    "-\tPrecision: 96.97 %\n",
    "-\tRecall: 91.94 %\n",
    "-\tF1-Score: 94.39 %\n",
    "\n",
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'cb_sgd_final.sav'\n",
    "joblib.dump(clf_sgd, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
